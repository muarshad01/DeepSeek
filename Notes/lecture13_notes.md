### MLA Code

* 15:00

***

* 20:00

* [B, S, D] = [1, s, 8] = [1, 1, 8]

***

* 25:00

***

* 30:00

***

* 35:00

***

* 40:00

***

* 45:00



concept. We don't just need to see the code. We need to write the matrices down on a piece of paper and work out
45:56
everything by hand from scratch. So the whole goal of today's lecture was to go
46:03
right down to the level of matrix multiplication and understand latent attention and parallelly write a code um
46:10
for the latent attention block. And I wanted to show you that it's not as hard if you understand what's going on
46:16
beneath the scenes if you can write it down on a piece of paper. And then I also wanted to show you main
46:22
demonstrations with respect to the fact that the latent attention does achieve a reduction in the KV cache memory and at
46:30
the same time it maintains language performance because it does not share any content across the different heads
46:35
and I hope I have been able to get these points across. Thanks a lot everyone. Please make notes as I'm speaking
46:42
because lectures are getting progressively more difficult. So to follow along it is extremely critical that you make detailed notes. In the
46:49
next lectures we'll be looking at rotary positional encoding. Then we'll modify the multi head latent attention with
46:55
rotary positional encoding. And then after that we'll look at mixture of experts multi-token prediction etc. So
47:02
lots of cool things are going to come. Thanks a lot everyone and I look forward to seeing you in the next lecture.















